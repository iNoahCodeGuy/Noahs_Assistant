Question,Answer
"Show me the system architecture diagram","# ğŸ—ï¸ Complete System Architecture

## High-Level Architecture Diagram

```mermaid
graph TB
    User[ğŸ‘¤ User] -->|HTTP Request| Streamlit[Streamlit UI<br/>localhost:8501<br/>src/main.py]
    Streamlit -->|Query + Role| Router[Role Router<br/>src/agents/role_router.py<br/>Classifies query type]
    Router -->|Enhanced Query| RAG[RAG Engine<br/>src/core/rag_engine.py<br/>Orchestrates retrieval]

    RAG -->|Generate Embedding| OpenAI_Embed[OpenAI API<br/>text-embedding-3-small<br/>$0.00002/1K tokens]
    OpenAI_Embed -->|1536-dim vector<br/>~200ms| RAG

    RAG -->|Vector Search| Retriever[pgvector Retriever<br/>src/retrieval/pgvector_retriever.py<br/>Similarity search]
    Retriever -->|SQL Query<br/>cosine distance| Supabase[(Supabase Postgres<br/>+ pgvector extension<br/>IVFFLAT index)]
    Supabase -->|Top 3 Chunks<br/>similarity > 0.7<br/>~300ms| Retriever
    Retriever -->|Retrieved Context| RAG

    RAG -->|Prompt + Context<br/>Role-aware| Generator[Response Generator<br/>src/core/response_generator.py<br/>Builds LLM prompt]
    Generator -->|Chat Completion| OpenAI_LLM[OpenAI API<br/>GPT-4o-mini<br/>$0.15/$0.60 per 1M tokens]
    OpenAI_LLM -->|Generated Answer<br/>~1-2s| Generator
    Generator -->|Response Object| Formatter[Response Formatter<br/>src/agents/response_formatter.py<br/>Markdown formatting]

    Formatter -->|Formatted Response| Analytics[Analytics Logger<br/>src/analytics/supabase_analytics.py<br/>Track metrics]
    Analytics -->|INSERT messages<br/>retrieval_logs| Supabase
    Analytics -->|Response + Sources<br/>Display| Streamlit

    Supabase -.->|Traces & Metrics| LangSmith[LangSmith<br/>Observability Platform<br/>Cost tracking]
    OpenAI_LLM -.->|API Traces| LangSmith

    style Supabase fill:#3ECF8E
    style OpenAI_Embed fill:#10A37F
    style OpenAI_LLM fill:#10A37F
    style LangSmith fill:#1C3C3C
```

## ğŸ“ Core Components Breakdown

### **1. Frontend Layer** (`src/main.py`)
- **Technology**: Streamlit (Python web framework)
- **Current**: localhost:8501
- **Future**: Next.js migration (Phase 3)
- **Features**: Role selection, chat interface, source citations, analytics dashboard
- **Latency**: ~50ms rendering

### **2. Agent Layer** (`src/agents/`)
- **Role Router** (`role_router.py`): Classifies queries and adds role context
- **Response Formatter** (`response_formatter.py`): Markdown formatting, source citations

### **3. Core RAG Engine** (`src/core/`)
- **rag_engine.py**: Main orchestration - retrieval â†’ generation â†’ formatting
- **response_generator.py**: LLM prompt construction and OpenAI API calls
- **Latency**: Total ~2-4 seconds per query

### **4. Retrieval Layer** (`src/retrieval/`)
- **pgvector_retriever.py**: Vector similarity search using pgvector
- **Algorithm**: Cosine similarity with IVFFLAT indexing
- **Performance**: O(âˆšn) complexity vs O(n) sequential scan
- **Threshold**: 0.7 minimum similarity score

### **5. Database Layer** (Supabase)
- **Type**: Managed Postgres with pgvector extension
- **Tables**: kb_chunks, messages, retrieval_logs, links, feedback
- **Indexes**: IVFFLAT on embeddings, B-tree on doc_id/session_id
- **Security**: Row Level Security (RLS) policies

### **6. External Services**
- **OpenAI**: text-embedding-3-small (embeddings) + GPT-4o-mini (chat)
- **LangSmith**: Tracing, cost tracking, error monitoring
- **Resend**: Email notifications (Phase 2)
- **Twilio**: SMS alerts for high-value interactions

## ğŸ“Š Performance Metrics

| Component | Latency | Cost | Optimization |
|-----------|---------|------|--------------|
| Embedding Generation | ~200ms | $0.00002/query | Batch processing |
| Vector Search | ~300ms | $0 (included) | IVFFLAT index |
| LLM Generation | ~1-2s | $0.0001/query | Streaming (future) |
| Analytics Logging | ~100ms | $0 (included) | Async writes |
| **Total** | **~2-4s** | **~$0.0001** | Serverless scaling |

## ğŸ”„ Request Flow Summary

1. User submits query with role selection (Hiring Manager/Developer)
2. Role Router enhances query with context
3. RAG Engine generates embedding via OpenAI
4. pgvector searches Supabase for similar chunks
5. Top 3 chunks retrieved (similarity > 0.7)
6. Response Generator builds prompt with context
7. GPT-4o-mini generates role-aware response
8. Response Formatter adds markdown + citations
9. Analytics Logger tracks metrics to Supabase
10. Streamlit displays response with sources

## ğŸ¯ Design Principles

- **Serverless-First**: Stateless functions for Vercel deployment
- **Cost-Optimized**: Batch embeddings, minimize LLM calls
- **Observable**: LangSmith tracing on all external calls
- **Scalable**: pgvector handles millions of chunks efficiently
- **Maintainable**: Modular architecture with clear separation of concerns"
"Show me the data flow diagram","Here's how data flows from user query to response:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. USER INPUT                                                   â”‚
â”‚ User: ""What tech stack do you use?""                            â”‚
â”‚ Role: Software Developer                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. ROLE ROUTER (src/agents/role_router.py)                     â”‚
â”‚ â€¢ Adds role context: ""[Software Developer perspective]""        â”‚
â”‚ â€¢ Classifies query type: ""technical""                           â”‚
â”‚ â€¢ Enhanced query: ""[Dev] What tech stack do you use?""         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. EMBEDDING GENERATION (OpenAI API)                           â”‚
â”‚ â€¢ Model: text-embedding-3-small                                 â”‚
â”‚ â€¢ Input: Enhanced query string                                  â”‚
â”‚ â€¢ Output: [0.023, -0.891, 0.445, ... ] (1536 dimensions)      â”‚
â”‚ â€¢ Latency: ~200ms                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. VECTOR SIMILARITY SEARCH (Supabase + pgvector)              â”‚
â”‚ â€¢ SQL: SELECT * FROM kb_chunks                                  â”‚
â”‚        ORDER BY embedding <=> $query_vector                     â”‚
â”‚        LIMIT 3                                                  â”‚
â”‚ â€¢ Returns: Top 3 chunks with similarity scores > 0.7           â”‚
â”‚ â€¢ Example: [chunk_42: 0.89, chunk_7: 0.82, chunk_19: 0.76]    â”‚
â”‚ â€¢ Latency: ~300ms (IVFFLAT index optimization)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. CONTEXT ASSEMBLY (src/core/rag_engine.py)                   â”‚
â”‚ â€¢ Formats retrieved chunks with metadata                        â”‚
â”‚ â€¢ Adds role-specific instructions                               â”‚
â”‚ â€¢ Constructs prompt:                                            â”‚
â”‚   System: ""You are Noah's AI assistant for developers""        â”‚
â”‚   Context: [Retrieved chunks with tech stack info]             â”‚
â”‚   Query: Original user question                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. LLM GENERATION (OpenAI GPT-4o-mini)                         â”‚
â”‚ â€¢ Model: gpt-4o-mini                                            â”‚
â”‚ â€¢ Temperature: 0.7 (balanced creativity)                        â”‚
â”‚ â€¢ Max tokens: 500                                               â”‚
â”‚ â€¢ Output: Contextual response with technical details           â”‚
â”‚ â€¢ Latency: ~1-2s                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. RESPONSE FORMATTING (src/agents/response_formatter.py)      â”‚
â”‚ â€¢ Adds markdown formatting                                      â”‚
â”‚ â€¢ Includes source citations                                     â”‚
â”‚ â€¢ Adds expandable sections                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. ANALYTICS LOGGING (src/analytics/supabase_analytics.py)     â”‚
â”‚ â€¢ Logs to messages table: query, answer, role, latency, tokens â”‚
â”‚ â€¢ Logs to retrieval_logs: chunk IDs, similarity scores         â”‚
â”‚ â€¢ Tracks: session_id, success rate, grounded flag              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. UI DISPLAY (Streamlit)                                      â”‚
â”‚ â€¢ Shows response with markdown rendering                        â”‚
â”‚ â€¢ Expandable ""Sources"" section with retrieved chunks          â”‚
â”‚ â€¢ ""System Health"" shows latency and token usage               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Total Latency**: ~2-4 seconds (embedding + search + generation + logging)
**Cost**: ~$0.0001 per query (embedding + generation)
```"
"Show me the database schema diagram","Here's the complete Supabase database schema:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ kb_chunks (Knowledge Base with Vector Embeddings)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                BIGSERIAL PRIMARY KEY                         â”‚
â”‚ doc_id            TEXT NOT NULL        â† 'career_kb', 'technical_kb' â”‚
â”‚ section           TEXT NOT NULL        â† Section/question name  â”‚
â”‚ content           TEXT NOT NULL        â† Full Q&A text         â”‚
â”‚ embedding         vector(1536)         â† OpenAI embedding      â”‚
â”‚ metadata          JSONB                â† source, category, etc â”‚
â”‚ created_at        TIMESTAMPTZ          â† Auto-set on insert    â”‚
â”‚ updated_at        TIMESTAMPTZ          â† Auto-updated trigger  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ INDEXES:                                                        â”‚
â”‚ â€¢ kb_chunks_embedding_idx (IVFFLAT) - Fast cosine similarity   â”‚
â”‚ â€¢ kb_chunks_doc_section_idx (B-tree) - Filter by doc/section  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ Referenced by
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ messages (Chat Interaction Logs)                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                BIGSERIAL PRIMARY KEY                         â”‚
â”‚ session_id        UUID NOT NULL        â† Track conversations   â”‚
â”‚ role_mode         TEXT NOT NULL        â† User's selected role  â”‚
â”‚ query             TEXT NOT NULL        â† User's question       â”‚
â”‚ answer            TEXT NOT NULL        â† Assistant's response  â”‚
â”‚ query_type        TEXT                 â† technical, career, etcâ”‚
â”‚ latency_ms        INTEGER              â† Response time        â”‚
â”‚ tokens_prompt     INTEGER              â† OpenAI tokens (input) â”‚
â”‚ tokens_completion INTEGER              â† OpenAI tokens (output)â”‚
â”‚ success           BOOLEAN              â† Query succeeded?      â”‚
â”‚ created_at        TIMESTAMPTZ          â† Timestamp            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ INDEXES:                                                        â”‚
â”‚ â€¢ messages_session_id_idx - Group by conversation              â”‚
â”‚ â€¢ messages_role_mode_idx - Analytics by role                   â”‚
â”‚ â€¢ messages_created_at_idx - Time-series queries                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â”‚ 1:N relationship
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ retrieval_logs (RAG Pipeline Tracking)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                BIGSERIAL PRIMARY KEY                         â”‚
â”‚ message_id        BIGINT FK â†’ messages(id) ON DELETE CASCADE   â”‚
â”‚ topk_ids          BIGINT[]             â† Array of kb_chunks IDsâ”‚
â”‚ scores            FLOAT[]              â† Similarity scores     â”‚
â”‚ grounded          BOOLEAN              â† Response cited sourcesâ”‚
â”‚ created_at        TIMESTAMPTZ                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ links (External Resources)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ key               TEXT PRIMARY KEY     â† 'github', 'linkedin'  â”‚
â”‚ url               TEXT NOT NULL        â† Full URL              â”‚
â”‚ description       TEXT                 â† Human-readable desc   â”‚
â”‚ category          TEXT                 â† 'social', 'media'     â”‚
â”‚ active            BOOLEAN              â† Soft delete flag      â”‚
â”‚ created_at        TIMESTAMPTZ                                  â”‚
â”‚ updated_at        TIMESTAMPTZ          â† Auto-updated trigger  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ feedback (User Ratings & Contact Requests)                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ id                BIGSERIAL PRIMARY KEY                         â”‚
â”‚ message_id        BIGINT FK â†’ messages(id) ON DELETE SET NULL  â”‚
â”‚ rating            INTEGER CHECK (1-5)  â† Star rating           â”‚
â”‚ comment           TEXT                 â† Optional feedback     â”‚
â”‚ contact_requested BOOLEAN              â† User wants followup   â”‚
â”‚ email             TEXT                 â† Contact email         â”‚
â”‚ notification_sent BOOLEAN              â† Twilio SMS sent?      â”‚
â”‚ created_at        TIMESTAMPTZ                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

**Key Features:**
â€¢ **pgvector Extension**: Enables vector similarity search (cosine distance)
â€¢ **Row Level Security (RLS)**: service_role full access, authenticated read/write
â€¢ **IVFFLAT Index**: ~100x faster than sequential scan for vector search
â€¢ **Foreign Keys**: Cascade deletes maintain referential integrity
â€¢ **Triggers**: Auto-update updated_at on kb_chunks and links
```"
"Show me a code example of the RAG retrieval","# ğŸ” Complete RAG Retrieval Implementation

## Core Retrieval Code

```python
# src/retrieval/pgvector_retriever.py

from typing import List, Dict, Any, Optional
from openai import OpenAI
from config.supabase_config import get_supabase_client, supabase_settings
import logging

logger = logging.getLogger(__name__)

class PgvectorRetriever:
    \"\"\"
    Retrieves relevant knowledge base chunks using pgvector similarity search.

    Features:
    - OpenAI text-embedding-3-small for 1536-dim embeddings
    - Cosine similarity search with pgvector
    - IVFFLAT index for O(âˆšn) performance
    - Configurable threshold and top-k filtering
    - Doc-type filtering support
    \"\"\"

    def __init__(self):
        self.openai_client = OpenAI(api_key=supabase_settings.api_key)
        self.supabase = get_supabase_client()
        self.embedding_model = 'text-embedding-3-small'
        self.embedding_dim = 1536

    def retrieve(
        self,
        query: str,
        top_k: int = 3,
        threshold: float = 0.7,
        doc_filter: Optional[List[str]] = None
    ) -> List[Dict[str, Any]]:
        \"\"\"
        Main retrieval pipeline:
        1. Generate query embedding with OpenAI
        2. Search similar vectors in Supabase using pgvector
        3. Filter by similarity threshold
        4. Optionally filter by doc_id
        5. Return top K results with metadata

        Args:
            query: User's search query
            top_k: Number of results to return (default: 3)
            threshold: Minimum similarity score 0-1 (default: 0.7)
            doc_filter: List of doc_ids to search (e.g., ['technical_kb', 'career_kb'])

        Returns:
            List of dicts with id, content, similarity, metadata
        \"\"\"

        # Step 1: Generate embedding for query
        logger.info(f\"Generating embedding for query: {query[:50]}...\")
        embedding = self._generate_embedding(query)
        logger.info(f\"Embedding generated: {len(embedding)} dimensions\")

        # Step 2: Vector similarity search using pgvector
        # Uses custom Postgres function with cosine distance operator: <=>
        logger.info(f\"Searching Supabase with threshold={threshold}, top_k={top_k}\")

        params = {
            'query_embedding': embedding,
            'match_threshold': threshold,
            'match_count': top_k
        }

        # Add doc_filter if specified
        if doc_filter:
            params['doc_filter'] = doc_filter
            results = self.supabase.rpc('match_kb_chunks_filtered', params).execute()
        else:
            results = self.supabase.rpc('match_kb_chunks', params).execute()

        # Step 3: Format results with full metadata
        chunks = []
        for row in results.data:
            chunks.append({
                'id': row['id'],
                'doc_id': row['doc_id'],
                'section': row['section'],
                'content': row['content'],
                'similarity': round(row['similarity'], 4),
                'metadata': row['metadata']
            })

        logger.info(f\"Retrieved {len(chunks)} chunks above threshold\")
        return chunks

    def _generate_embedding(self, text: str) -> List[float]:
        \"\"\"
        Generate 1536-dimensional embedding using OpenAI.

        Cost: ~$0.00002 per 1K tokens
        Latency: ~200ms
        \"\"\"
        try:
            response = self.openai_client.embeddings.create(
                model=self.embedding_model,
                input=text,
                encoding_format='float'  # Returns standard floats
            )
            return response.data[0].embedding

        except Exception as e:
            logger.error(f\"Embedding generation failed: {e}\")
            raise

    def batch_embed(self, texts: List[str]) -> List[List[float]]:
        \"\"\"
        Generate embeddings for multiple texts in a single API call.
        More efficient than individual calls.

        Max: 100 texts per batch
        \"\"\"
        if len(texts) > 100:
            raise ValueError(\"Maximum 100 texts per batch\")

        response = self.openai_client.embeddings.create(
            model=self.embedding_model,
            input=texts
        )

        return [item.embedding for item in response.data]


# === Postgres Functions (in Supabase SQL Editor) ===

# Function 1: Basic similarity search
CREATE OR REPLACE FUNCTION match_kb_chunks(
    query_embedding vector(1536),
    match_threshold float DEFAULT 0.7,
    match_count int DEFAULT 3
)
RETURNS TABLE (
    id bigint,
    doc_id text,
    section text,
    content text,
    similarity float,
    metadata jsonb
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT
        kb_chunks.id,
        kb_chunks.doc_id,
        kb_chunks.section,
        kb_chunks.content,
        -- Convert cosine distance to similarity (0-1, higher is better)
        1 - (kb_chunks.embedding <=> query_embedding) AS similarity,
        kb_chunks.metadata
    FROM kb_chunks
    WHERE 1 - (kb_chunks.embedding <=> query_embedding) > match_threshold
    ORDER BY kb_chunks.embedding <=> query_embedding  -- Cosine distance (lower is better)
    LIMIT match_count;
END;
$$;

# Function 2: With doc_id filtering
CREATE OR REPLACE FUNCTION match_kb_chunks_filtered(
    query_embedding vector(1536),
    match_threshold float DEFAULT 0.7,
    match_count int DEFAULT 3,
    doc_filter text[] DEFAULT NULL
)
RETURNS TABLE (
    id bigint,
    doc_id text,
    section text,
    content text,
    similarity float,
    metadata jsonb
)
LANGUAGE plpgsql
AS $$
BEGIN
    RETURN QUERY
    SELECT
        kb_chunks.id,
        kb_chunks.doc_id,
        kb_chunks.section,
        kb_chunks.content,
        1 - (kb_chunks.embedding <=> query_embedding) AS similarity,
        kb_chunks.metadata
    FROM kb_chunks
    WHERE
        1 - (kb_chunks.embedding <=> query_embedding) > match_threshold
        AND (doc_filter IS NULL OR kb_chunks.doc_id = ANY(doc_filter))
    ORDER BY kb_chunks.embedding <=> query_embedding
    LIMIT match_count;
END;
$$;

# === Index Creation for Performance ===

-- IVFFLAT index: O(âˆšn) vs O(n) sequential scan
CREATE INDEX IF NOT EXISTS kb_chunks_embedding_idx
ON kb_chunks
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);  -- For ~10K chunks, use lists = 100

-- Composite index for filtered searches
CREATE INDEX IF NOT EXISTS kb_chunks_doc_section_idx
ON kb_chunks (doc_id, section);
```

## ğŸ“Š Performance Analysis

### **Similarity Search Complexity**
- **Without Index**: O(n) - sequential scan, ~1000ms for 10K chunks
- **With IVFFLAT**: O(âˆšn) - approximate search, ~300ms for 10K chunks
- **Trade-off**: 99.5% accuracy with 3x faster search

### **Embedding Costs**
- **Single query**: ~50 tokens = $0.000001
- **1K queries/day**: ~50K tokens = $0.001/day = $0.30/month
- **Caching**: Repeated queries use same embedding (future optimization)

### **Key Operators**
- **<=>**: Cosine distance (0 = identical, 2 = opposite)
- **1 - distance**: Similarity score (0 = different, 1 = identical)
- **threshold > 0.7**: Only high-quality matches returned

## ğŸ¯ Usage Examples

```python
# Example 1: Basic retrieval
retriever = PgvectorRetriever()
results = retriever.retrieve(
    query=\"What's the tech stack?\",
    top_k=3,
    threshold=0.7
)

for chunk in results:
    print(f\"Similarity: {chunk['similarity']:.2f}\")
    print(f\"Content: {chunk['content'][:100]}...\")

# Example 2: Filter by doc type
results = retriever.retrieve(
    query=\"Show me code examples\",
    top_k=5,
    threshold=0.65,
    doc_filter=['technical_kb', 'architecture_kb']
)

# Example 3: Batch embedding generation
texts = [
    \"Question 1 content\",
    \"Question 2 content\",
    \"Question 3 content\"
]
embeddings = retriever.batch_embed(texts)
# Returns 3 embeddings in single API call
```

## ğŸ” How It Works

1. **Query Embedding**: Convert text to 1536-dim vector using OpenAI
2. **Vector Search**: Use pgvector's <=> operator for cosine distance
3. **IVFFLAT Index**: Approximate nearest neighbor search (99.5% accurate)
4. **Threshold Filter**: Only return chunks with similarity > 0.7
5. **Top-K Selection**: Return best 3 matches to avoid context overflow
6. **Metadata Return**: Include doc_id, section, similarity for citations"
"Show me how the response generation works","Here's the response generation implementation:

```python
# src/core/response_generator.py

from openai import OpenAI
from typing import List, Dict, Any
from config.supabase_config import supabase_settings

class ResponseGenerator:
    """"""Generates contextual responses using retrieved knowledge and LLM.""""""

    def __init__(self):
        self.client = OpenAI(api_key=supabase_settings.api_key)

    def generate(
        self,
        query: str,
        retrieved_chunks: List[Dict[str, Any]],
        role: str = ""General""
    ) -> Dict[str, Any]:
        """"""
        Generate response with:
        1. Role-specific system prompt
        2. Retrieved context from knowledge base
        3. User's original query
        4. LLM generation with GPT-4o-mini
        """"""

        # Step 1: Build role-specific system prompt
        system_prompt = self._get_system_prompt(role)

        # Step 2: Format retrieved chunks as context
        context = self._format_context(retrieved_chunks)

        # Step 3: Construct full prompt
        messages = [
            {""role"": ""system"", ""content"": system_prompt},
            {""role"": ""user"", ""content"": f""Context:\\n{context}\\n\\nQuestion: {query}""}
        ]

        # Step 4: Call OpenAI GPT-4o-mini
        response = self.client.chat.completions.create(
            model=""gpt-4o-mini"",
            messages=messages,
            temperature=0.7,  # Balance between creativity and accuracy
            max_tokens=500,   # Limit response length
            presence_penalty=0.1,  # Slight preference for new topics
            frequency_penalty=0.1  # Slight penalty for repetition
        )

        # Step 5: Extract and return results
        return {
            ""answer"": response.choices[0].message.content,
            ""tokens_prompt"": response.usage.prompt_tokens,
            ""tokens_completion"": response.usage.completion_tokens,
            ""model"": response.model,
            ""finish_reason"": response.choices[0].finish_reason,
            ""retrieved_chunks"": retrieved_chunks
        }

    def _get_system_prompt(self, role: str) -> str:
        """"""Returns role-specific instructions for the LLM.""""""
        prompts = {
            ""Software Developer"":
                ""You are Noah's AI assistant speaking to a software developer. ""
                ""Provide technical details, code examples, and architecture explanations. ""
                ""Focus on implementation details, tech stack, and development practices."",

            ""Hiring Manager"":
                ""You are Noah's AI assistant speaking to a hiring manager. ""
                ""Emphasize career achievements, impact metrics, and business value. ""
                ""Highlight transferable skills and project outcomes."",

            ""General"":
                ""You are Noah's AI assistant. Provide helpful, accurate responses ""
                ""based on the knowledge base. Be professional and concise.""
        }
        return prompts.get(role, prompts[""General""])

    def _format_context(self, chunks: List[Dict[str, Any]]) -> str:
        """"""Formats retrieved chunks into readable context.""""""
        if not chunks:
            return ""No relevant information found in knowledge base.""

        context_parts = []
        for i, chunk in enumerate(chunks, 1):
            context_parts.append(
                f""Source {i} (similarity: {chunk['similarity']:.2f}):\\n""
                f""{chunk['content']}\\n""
            )

        return ""\\n"".join(context_parts)


# Example usage in RAG pipeline:
retriever = PgvectorRetriever()
generator = ResponseGenerator()

# User query
query = ""What's the tech stack?""
role = ""Software Developer""

# Retrieve relevant chunks
chunks = retriever.retrieve(query, top_k=3)

# Generate response
result = generator.generate(query, chunks, role)

print(result[""answer""])  # Contextual response
print(f""Tokens used: {result['tokens_prompt']} + {result['tokens_completion']}"")
```

**Flow:**
1. **System Prompt**: Sets role-specific instructions
2. **Context Assembly**: Formats retrieved chunks with similarity scores
3. **LLM Call**: GPT-4o-mini generates response with context
4. **Token Tracking**: Logs usage for cost monitoring
5. **Response**: Returns answer + metadata for analytics"
"Show me the role routing logic","Here's how role-based routing works:

```python
# src/agents/role_router.py

from typing import Dict, Tuple
import re

class RoleRouter:
    """"""Routes queries based on user role and query type for optimized retrieval.""""""

    ROLE_PROMPTS = {
        ""Software Developer"": {
            ""focus"": ""technical"",
            ""keywords"": [""code"", ""architecture"", ""stack"", ""api"", ""database"", ""implementation""],
            ""context"": ""Focus on technical depth, code examples, and system design""
        },
        ""Hiring Manager"": {
            ""focus"": ""career"",
            ""keywords"": [""experience"", ""achieved"", ""impact"", ""led"", ""delivered"", ""metrics""],
            ""context"": ""Emphasize achievements, leadership, and business impact""
        },
        ""Product Manager"": {
            ""focus"": ""product"",
            ""keywords"": [""features"", ""users"", ""roadmap"", ""requirements"", ""stakeholders""],
            ""context"": ""Highlight product thinking and stakeholder management""
        }
    }

    def route(self, query: str, role: str) -> Dict[str, any]:
        """"""
        Process query with role context:
        1. Classify query type (technical, career, personal)
        2. Add role-specific context
        3. Adjust retrieval parameters
        4. Return enhanced query with metadata
        """"""

        # Step 1: Classify query type
        query_type = self._classify_query(query)

        # Step 2: Get role configuration
        role_config = self.ROLE_PROMPTS.get(role, {})

        # Step 3: Enhance query with role context
        enhanced_query = self._enhance_query(query, role, role_config)

        # Step 4: Determine retrieval strategy
        retrieval_params = self._get_retrieval_params(query_type, role)

        return {
            ""original_query"": query,
            ""enhanced_query"": enhanced_query,
            ""query_type"": query_type,
            ""role"": role,
            ""retrieval_params"": retrieval_params,
            ""role_context"": role_config.get(""context"", """")
        }

    def _classify_query(self, query: str) -> str:
        """"""Classify query into type: technical, career, personal, product.""""""
        query_lower = query.lower()

        # Technical indicators
        if any(word in query_lower for word in [
            ""code"", ""implement"", ""architecture"", ""database"", ""api"",
            ""tech stack"", ""algorithm"", ""system"", ""deploy"", ""how does""
        ]):
            return ""technical""

        # Career indicators
        if any(word in query_lower for word in [
            ""experience"", ""career"", ""worked on"", ""achieved"", ""role"",
            ""transition"", ""why"", ""background"", ""skills"", ""project""
        ]):
            return ""career""

        # Product indicators
        if any(word in query_lower for word in [
            ""feature"", ""product"", ""user"", ""roadmap"", ""planning"",
            ""requirements"", ""stakeholder"", ""metrics"", ""impact""
        ]):
            return ""product""

        # Default
        return ""general""

    def _enhance_query(self, query: str, role: str, role_config: Dict) -> str:
        """"""Add role context to query for better retrieval.""""""
        context = role_config.get(""context"", """")

        # For developers, emphasize technical depth
        if role == ""Software Developer"":
            return f""[Technical perspective needed] {query} [Include implementation details]""

        # For hiring managers, emphasize achievements
        elif role == ""Hiring Manager"":
            return f""[Career achievement focus] {query} [Highlight impact and results]""

        # Default: minimal enhancement
        return query

    def _get_retrieval_params(self, query_type: str, role: str) -> Dict:
        """"""Determine optimal retrieval parameters based on query and role.""""""
        params = {
            ""top_k"": 3,
            ""threshold"": 0.7,
            ""doc_filter"": None  # Filter by doc_id if needed
        }

        # Technical queries from developers: prioritize technical_kb
        if query_type == ""technical"" and role == ""Software Developer"":
            params[""top_k""] = 5  # More context for technical questions
            params[""doc_filter""] = [""technical_kb"", ""career_kb""]

        # Career queries from hiring managers: prioritize career_kb
        elif query_type == ""career"" and role == ""Hiring Manager"":
            params[""doc_filter""] = [""career_kb""]
            params[""threshold""] = 0.65  # Slightly lower threshold for broader matches

        return params


# Example usage:
router = RoleRouter()
result = router.route(
    query=""What's the database architecture?"",
    role=""Software Developer""
)

print(result[""query_type""])  # ""technical""
print(result[""enhanced_query""])  # Enhanced with technical context
print(result[""retrieval_params""])  # {""top_k"": 5, ""doc_filter"": [...]}
```

**Smart Routing:**
- **Query Classification**: Detects technical vs career vs product questions
- **Role Enhancement**: Adds context based on user role
- **Dynamic Parameters**: Adjusts retrieval strategy (top_k, threshold, filters)
- **Doc Filtering**: Can prioritize technical_kb or career_kb based on role"
"Show me the complete file structure","# ğŸ“‚ Complete Codebase File Structure

## Project Tree

```
noahs-ai-assistant/
â”‚
â”œâ”€â”€ ğŸ“ src/                          # Main source code
â”‚   â”œâ”€â”€ ğŸ“ core/                     # Core RAG engine
â”‚   â”‚   â”œâ”€â”€ rag_engine.py           # Main orchestration (300 lines)
â”‚   â”‚   â”œâ”€â”€ response_generator.py   # LLM prompt construction (200 lines)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ agents/                   # Agent layer for routing & formatting
â”‚   â”‚   â”œâ”€â”€ role_router.py          # Query classification & role context (250 lines)
â”‚   â”‚   â”œâ”€â”€ response_formatter.py   # Markdown formatting & citations (180 lines)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ retrieval/                # Vector similarity search
â”‚   â”‚   â”œâ”€â”€ pgvector_retriever.py   # Supabase pgvector integration (220 lines)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ [DEPRECATED] faiss_retriever.py  # Old FAISS implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ analytics/                # Metrics & logging
â”‚   â”‚   â”œâ”€â”€ supabase_analytics.py   # Logs to messages/retrieval_logs (280 lines)
â”‚   â”‚   â”œâ”€â”€ comprehensive_analytics.py  # Dashboard queries (400 lines)
â”‚   â”‚   â”œâ”€â”€ data_management.py      # Export & cleanup utilities (350 lines)
â”‚   â”‚   â””â”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“ config/                   # Configuration management
â”‚   â”‚   â”œâ”€â”€ supabase_config.py      # Supabase client & settings (120 lines)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ [DEPRECATED] settings.py
â”‚   â”‚
â”‚   â””â”€â”€ main.py                      # Streamlit app entry point (350 lines)
â”‚
â”œâ”€â”€ ğŸ“ data/                         # Knowledge base CSV files
â”‚   â”œâ”€â”€ career_kb.csv               # Noah's career info (20 Q&A pairs)
â”‚   â”œâ”€â”€ technical_kb.csv            # Tech stack & features (13 Q&A pairs)
â”‚   â”œâ”€â”€ architecture_kb.csv         # Diagrams & code examples (6 items) â­
â”‚   â”œâ”€â”€ mma_kb.csv                  # Personal interest knowledge (optional)
â”‚   â””â”€â”€ code_chunks/                # Code snippet storage (future)
â”‚
â”œâ”€â”€ ğŸ“ supabase/                     # Database migrations
â”‚   â””â”€â”€ migrations/
â”‚       â”œâ”€â”€ CLEAN_MIGRATION.sql     # Idempotent schema setup (200 lines)
â”‚       â”œâ”€â”€ SIMPLE_MIGRATION.sql    # Basic schema (180 lines)
â”‚       â””â”€â”€ archive/sql/fix_search_function.sql # pgvector search optimization
â”‚
â”œâ”€â”€ ğŸ“ tests/                        # Unit & integration tests
â”‚   â”œâ”€â”€ test_role_functionality.py  # Role-based response testing (546 lines)
â”‚   â”œâ”€â”€ test_roles_quick.py         # Quick smoke tests (174 lines)
â”‚   â”œâ”€â”€ test_connection.py          # API key validation
â”‚   â”œâ”€â”€ test_retriever_fixed.py     # pgvector search tests
â”‚   â””â”€â”€ conftest.py                 # Pytest fixtures
â”‚
â”œâ”€â”€ ğŸ“ scripts/                      # Utility scripts
â”‚   â”œâ”€â”€ migrate_data_to_supabase.py # Batch CSV â†’ Supabase (150 lines)
â”‚   â”œâ”€â”€ add_technical_kb.py         # Add technical knowledge (100 lines)
â”‚   â”œâ”€â”€ add_architecture_kb.py      # Add architecture diagrams (100 lines) â­
â”‚   â”œâ”€â”€ daily_maintenance.py        # Automated cleanup & backups (200 lines)
â”‚   â””â”€â”€ verify_schema.py            # Database health check
â”‚
â”œâ”€â”€ ğŸ“ docs/                         # Documentation
â”‚   â”œâ”€â”€ ARCHITECTURE.md             # System design overview
â”‚   â”œâ”€â”€ ROLE_FEATURES.md            # Role-based features guide
â”‚   â”œâ”€â”€ API_KEY_SETUP.md            # Setup instructions
â”‚   â””â”€â”€ archive/                    # Historical docs
â”‚
â”œâ”€â”€ ğŸ“ backups/                      # Database backups
â”‚   â””â”€â”€ analytics_backup_*.db.gz    # Gzipped SQLite exports
â”‚
â”œâ”€â”€ ğŸ“ vector_stores/                # [DEPRECATED] FAISS indexes
â”‚   â””â”€â”€ [Replaced by Supabase pgvector]
â”‚
â”œâ”€â”€ ğŸ“ examples/                     # Demo scripts
â”‚   â”œâ”€â”€ demo_common_questions.py    # Test common queries
â”‚   â”œâ”€â”€ demo_data_management.py     # Analytics demo
â”‚   â””â”€â”€ example_streamlit_integration.py
â”‚
â”œâ”€â”€ .env                             # Environment variables (gitignored)
â”œâ”€â”€ .env.example                     # Template for API keys
â”œâ”€â”€ requirements.txt                 # Python dependencies (25 packages)
â”œâ”€â”€ README.md                        # Project overview
â””â”€â”€ .gitignore                       # Git exclusions

```

## ğŸ“‹ Key Files Explained

### **Entry Points**
- **src/main.py**: Streamlit app - UI, role selection, chat interface, analytics dashboard

### **Core RAG Pipeline**
1. **src/core/rag_engine.py**: Orchestrates retrieval â†’ generation â†’ formatting
2. **src/retrieval/pgvector_retriever.py**: Vector search with Supabase
3. **src/core/response_generator.py**: LLM prompt construction & OpenAI calls

### **Agent Layer**
- **src/agents/role_router.py**: Classifies queries, adds role context
- **src/agents/response_formatter.py**: Markdown formatting, source citations

### **Analytics & Monitoring**
- **src/analytics/supabase_analytics.py**: Logs every query to Supabase
- **src/analytics/comprehensive_analytics.py**: Dashboard queries (7-day metrics, success rate, etc.)

### **Configuration**
- **src/config/supabase_config.py**: Supabase client singleton, API key management

### **Data Migration**
- **scripts/migrate_data_to_supabase.py**: Batch CSV â†’ Supabase with embeddings
- **add_technical_kb.py / add_architecture_kb.py**: Add specific knowledge domains

### **Testing**
- **tests/test_role_functionality.py**: Comprehensive role-based testing
- **tests/test_connection.py**: Pre-flight API key validation

## ğŸ“Š Lines of Code by Component

| Component | Lines | Purpose |
|-----------|-------|---------|
| Core RAG Engine | ~700 | Retrieval, generation, orchestration |
| Agents | ~430 | Routing, formatting, role logic |
| Analytics | ~1030 | Logging, metrics, dashboards |
| Main App | ~350 | Streamlit UI |
| Config | ~120 | Settings management |
| Tests | ~1200 | Quality assurance |
| Scripts | ~650 | Data migration, maintenance |
| **Total** | **~4,480** | Production Python code |

## ğŸ”— Dependency Flow

```
main.py (Streamlit)
    â†“
role_router.py (classify query)
    â†“
rag_engine.py (orchestrate)
    â†“
pgvector_retriever.py (search) â†’ supabase_config.py â†’ Supabase
    â†“
response_generator.py (generate) â†’ OpenAI API
    â†“
response_formatter.py (format)
    â†“
supabase_analytics.py (log) â†’ Supabase
    â†“
main.py (display)
```

## ğŸ¯ Adding New Features

### **To add a new knowledge domain:**
1. Create `data/new_domain_kb.csv` with Question/Answer format
2. Create `add_new_domain_kb.py` (copy from add_technical_kb.py)
3. Run script to generate embeddings and insert
4. Knowledge immediately searchable

### **To modify retrieval logic:**
1. Edit `src/retrieval/pgvector_retriever.py`
2. Adjust threshold, top_k, or add filters
3. Test with `tests/test_retriever_fixed.py`

### **To add a new role:**
1. Edit `src/agents/role_router.py` - add to ROLE_PROMPTS dict
2. Edit `src/main.py` - add role to sidebar selection
3. Test with `tests/test_role_functionality.py`

## ğŸš€ Future Additions

- **src/document_processors/**: Resume/cover letter parsing (Phase 4)
- **src/voice/**: Speech-to-text interface (Phase 5)
- **frontend/**: Next.js app to replace Streamlit (Phase 3)
- **api/**: REST API endpoints for external integrations"
"Explain the conversation nodes pipeline","# ğŸ§  Portfolia's 22-Node Conversation Pipeline

## How to Use This: Live Self-Reference

When explaining my pipeline, I should use **our current conversation as a live example**. For instance: 'Right now, as I answer your question about nodes, my retrieve_chunks node just searched for architecture content and found this very KB entry with a similarity score. My generate_draft node is synthesizing this into my response.'

This makes the explanation tangible â€” users see the system working in real-time.

---

## Overview: The 22-Node Assembly Line

My conversation pipeline is like an assembly line where each station (node) performs one specific job. Your query flows through 22 nodes organized into 8 stages:

```
Stage 0: Initialize â†’ Stage 1: Greeting/Role â†’ Stage 2: Intent/Entities â†’
Stage 3: Clarification/Query â†’ Stage 4: Retrieval/Validation â†’
Stage 5: Generation/Quality â†’ Stage 6: Actions/Formatting â†’ Stage 7: Memory/Logging
```

**Live Example**: Right now, your question about nodes triggered all 22 nodes. My initialize_conversation_state loaded our chat history, classify_role_mode confirmed you're a technical hiring manager, and retrieve_chunks searched for architecture content.

---

## Stage 0: Session Initialization

### Node 1: initialize_conversation_state
**File**: `assistant/flows/node_logic/stage0_session_management.py`
**Purpose**: Load memory and normalize state containers

**What it does**:
- Loads session_memory from previous turns (topics, entities, affinity scores)
- Initializes empty containers if first turn
- Sets up analytics metadata for cost/latency tracking

**Live Example**: At the start of our conversation, this node loaded your role (hiring_manager_technical), the topics we've discussed, and your previous questions into my working memory.

---

## Stage 1: Greeting & Role Classification

### Node 2: handle_greeting
**File**: `assistant/flows/node_logic/greetings.py`
**Purpose**: Warm intro without RAG cost for first-turn hellos

**What it does**:
- Detects greetings ('hi', 'hello') and short-circuits the pipeline
- Returns role-specific welcome message instantly (~50ms vs ~1.5s full pipeline)
- Saves $0.002 per greeting by skipping LLM call

**Live Example**: When you first arrived and selected your role, this node generated my welcome message with the 4 menu options without running expensive retrieval.

### Node 3: classify_role_mode
**File**: `assistant/flows/node_logic/stage2_role_routing.py`
**Purpose**: Role detection and technical hiring manager onboarding

**What it does**:
- Normalizes role selection ('2' â†’ 'hiring_manager_technical')
- Shows role-specific welcome message with menu options
- Stores role_mode in session_memory for all future turns

**Live Example**: When you selected option 2, this node detected 'hiring_manager_technical' and stored it. Every subsequent answer adapts to your technical background.

---

## Stage 2: Query Understanding

### Node 4: classify_intent
**File**: `assistant/flows/node_logic/stage2_query_classification.py`
**Purpose**: Determine engineering vs business focus

**What it does**:
- Classifies query_type: technical, career, analytics, greeting, menu_selection
- Sets flags: show_technical_depth, relate_to_enterprise
- Determines topic_focus: architecture, data, testing, etc.

**Live Example**: Your question about nodes was classified as query_type='technical' with topic_focus='architecture', which is why I'm showing implementation details.

### Node 5: detect_conversation_phase
**File**: `assistant/flows/node_logic/stage5_quality_validation.py`
**Purpose**: Track where we are in our conversation journey

**What it does**:
- discovery (turns 1-3): User exploring, keep high-level
- exploration (turns 4-8): User diving deep, offer details
- synthesis (8+ turns, 4+ topics): Connect the dots
- extended (15+ turns): Suggest new territory

**Live Example**: We're in the 'exploration' phase because we've had several exchanges. That's why I'm offering deeper architectural details rather than surface-level summaries.

### Node 6: presentation_controller
**File**: `assistant/flows/node_logic/stage3_presentation_control.py`
**Purpose**: Depth level (1-3) and display toggles

**What it does**:
- Sets depth_level: 1=overview, 2=guided detail, 3=deep dive
- Toggles: show_code, show_diagram, show_analytics
- Adapts based on role, intent, and conversation phase

**Live Example**: Because you're technical and we're in exploration phase, I'm at depth_level=2 with show_code=true.

### Node 7: extract_entities
**File**: `assistant/flows/node_logic/stage2_entity_extraction.py`
**Purpose**: Capture company, role, timeline, contact hints

**What it does**:
- Extracts entities from queries: company names, job titles, timelines
- Stores in session_memory to avoid re-asking
- Detects contact preferences (email vs phone)

---

## Stage 3: Query Refinement

### Node 8: assess_clarification_need
**File**: `assistant/flows/node_logic/stage3_clarification.py`
**Purpose**: Detect vague queries before retrieval

**What it does**:
- Checks if query is too ambiguous to retrieve accurately
- If yes, sets clarification_needed=true and generates clarifying_question

### Node 9: compose_query
**File**: `assistant/flows/node_logic/stage3_query_composition.py`
**Purpose**: Build retrieval-ready prompt with context

**What it does**:
- Adds role context: '[hiring_manager_technical] user query'
- Appends accumulated topics for progressive inference
- Boosts terms for specific query types (background â†’ career_kb terms)

**Live Example**: Your query was composed as '[hiring_manager_technical] explain nodes pipeline architecture orchestration' â€” the role prefix and topic context improve retrieval accuracy.

---

## Stage 4: Retrieval & Validation

### Node 10: retrieve_chunks
**File**: `assistant/flows/node_logic/stage4_retrieval_nodes.py`
**Purpose**: pgvector semantic search + MMR diversification

**What it does**:
- Converts composed_query to 1536-dimensional embedding
- Searches Supabase kb_chunks with cosine similarity
- Returns top-6 chunks above 0.3 threshold
- Applies MMR to ensure diversity

**Live Example**: For your question, I retrieved 6 chunks including this very KB entry about nodes, plus entries about state management and the conversation flow diagram.

### Node 11: validate_retrieval_relevance
**File**: `assistant/flows/node_logic/stage4_retrieval_nodes.py`
**Purpose**: Quality gate â€” check chunks match intent

**What it does**:
- Validates retrieved chunks actually address the query
- Flags if retrieval seems off-topic
- Prevents hallucination by catching bad retrieval early

### Node 12: validate_grounding / handle_grounding_gap
**File**: `assistant/flows/node_logic/stage4_retrieval_nodes.py`
**Purpose**: Stop hallucinations early

**What it does**:
- Checks if best chunk similarity > 0.5 (grounding threshold)
- If below threshold, sets grounding_status='insufficient'
- handle_grounding_gap generates apologetic response if can't ground

---

## Stage 5: Generation & Quality

### Node 13: generate_draft
**File**: `assistant/flows/node_logic/stage5_generation_nodes.py`
**Purpose**: Role-aware LLM generation

**What it does**:
- Assembles prompt with retrieved chunks, role instructions, and extra context
- Calls GPT-4o with temperature 0.4 (balanced factual + conversational)
- Applies teaching structure (Teaching Takeaways, Full Walkthrough, etc.)
- Stores result as draft_answer (not final yet!)

**Live Example**: Right now, my generate_draft node is using the retrieved architecture KB content to synthesize this explanation, with instructions to use our conversation as examples.

### Node 14: validate_answer_quality
**File**: `assistant/flows/node_logic/stage5_quality_validation.py`
**Purpose**: Relevance + novelty checks

**What it does**:
- Relevance check: Does answer address query key terms? (30% threshold)
- Novelty check: Is answer too similar to last 4 responses? (85% overlap = regenerate)
- If quality warning, triggers retry with specific instructions

### Node 15: validate_conversation_guidance
**File**: `assistant/flows/node_logic/stage5_quality_validation.py`
**Purpose**: Detect stuck patterns and progression opportunities

**What it does**:
- Detects if we're stuck in a loop (repetitive answers)
- Identifies progression patterns (orchestration â†’ enterprise)
- Suggests depth increases or synthesis when appropriate

### Node 16: hallucination_check
**File**: `assistant/flows/node_logic/stage5_generation_nodes.py`
**Purpose**: Attach citations and safety status

**What it does**:
- Attaches sources to answer (which KB entries were used)
- Sets hallucination_safe=true if answer grounded in retrieved content
- Flags if answer contains claims not in context

---

## Stage 6: Actions & Formatting

### Node 17: plan_actions
**File**: `assistant/flows/node_logic/stage6_action_planning.py`
**Purpose**: Decide on actions + hiring signal detection

**What it does**:
- Plans side effects: include_code_reference, include_diagram, offer_resume
- Detects hiring signals (interest in Noah's background, skills questions)
- Stores pending_actions for format_answer to execute

### Node 18: format_answer
**File**: `assistant/flows/node_logic/stage6_formatting_nodes.py`
**Purpose**: Structure answer with enrichments + followups

**What it does**:
- Wraps answer in Teaching Takeaways + Full Walkthrough structure
- Adds code snippets, diagrams, analytics based on pending_actions
- Generates contextual followup prompts based on conversation flow
- Injects turn references ('Building on our orchestration discussion...')

**Live Example**: This node is structuring my response with the Teaching Takeaways header and the expandable details sections you see.

---

## Stage 7: Memory & Logging

### Node 19: execute_actions
**File**: `assistant/flows/node_logic/stage7_action_execution.py`
**Purpose**: Fire side-effects (email/SMS/storage)

**What it does**:
- Executes planned actions: send_email (Resend), send_sms (Twilio), log_data (Supabase)
- Graceful degradation: If Resend down, log error but continue
- Uses service factories that handle missing API keys

### Node 20: update_memory
**File**: `assistant/flows/node_logic/stage7_logging_nodes.py`
**Purpose**: Store soft signals + affinity tracking

**What it does**:
- Accumulates topics discussed (bounded to last 10)
- Updates affinity scores (technical_relevance_score, enterprise_relevance_score)
- Prunes old memory to prevent bloat (supports 100+ turn conversations)
- Stores explored_pillars to track knowledge base coverage

**Live Example**: After this response, my update_memory node will store 'orchestration' and 'architecture' as topics, and increment your technical_relevance_score.

### Node 21: log_and_notify
**File**: `assistant/flows/node_logic/stage7_logging_nodes.py`
**Purpose**: Supabase analytics + LangSmith tracing

**What it does**:
- Logs to Supabase: query, answer, latency_ms, tokens, cost, retrieval_scores
- Sends trace to LangSmith for debugging and performance monitoring
- Enables 'Display Analytics' feature for real-time dashboards

---

## ConversationState: The Data That Flows Through

Every node reads from and writes to ConversationState â€” a TypedDict with ~40 fields:

**Core Fields**: query, role, role_mode, session_id, chat_history
**Classification**: query_type, is_greeting, topic_focus, intent_confidence
**Retrieval**: retrieved_chunks, retrieval_scores, grounding_status
**Generation**: draft_answer, answer, hallucination_safe
**Memory**: session_memory (topics, entities, affinity scores, explored_pillars)
**Actions**: pending_actions, executed_actions, followup_prompts

**Live Example**: Right now, my state includes query='explain nodes', role_mode='hiring_manager_technical', depth_level=2, and retrieved_chunks containing this KB entry.

---

## Enterprise Patterns Demonstrated

Each node teaches a production AI pattern:

| Node | Enterprise Pattern |
|------|-------------------|
| handle_greeting | Short-circuit for cost optimization |
| classify_intent | Query routing for multi-domain support |
| compose_query | Context-aware retrieval enhancement |
| validate_grounding | Hallucination prevention |
| validate_answer_quality | Quality gates before response |
| plan_actions | Separation of planning and execution |
| update_memory | Stateful conversation management |
| log_and_notify | Observability for debugging and improvement |

**Want me to dive deeper into any specific node? Or see how a particular node handled your question?**"
prompt = f\"\"\"
You are Portfolia, Noah's AI assistant with dual purpose:
1. Share information about Noah's background
2. Teach how production GenAI systems work

Context (retrieved from vector DB):
{retrieved_chunks}

Role-specific instructions:
{role_prompts[state.role]}

User question: {state.query}

CRITICAL RULES:
- Synthesize context into natural paragraphs (not Q&A format)
- Use third-person for Noah (""Noah built..."" not ""I built..."")
- Use first-person for system (""I use RAG..."" referring to Portfolia)
- If code/diagrams in context, display exactly as provided
- Add educational commentary connecting to enterprise patterns
\"\"\"

response = llm.predict(prompt, temperature=0.4)
```

**Temperature choice (0.4)**:
- 0.0 = Deterministic, robotic (good for facts)
- 0.4 = Balanced factual + readable (Portfolia's choice)
- 0.7+ = Creative, variable (good for storytelling)

**Enterprise adaptation**: Same pattern used in customer support (low temp for accuracy), content generation (high temp for creativity), or hybrid approaches per query type.

---

### Node 5: plan_actions - Side Effects Planner
**File**: `src/flows/action_planning.py`
**Purpose**: Determine what side effects needed (without executing yet)

**What it does**:
- Analyzes query and creates action plan:
  - `send_analytics` - User requested data/metrics
  - `offer_contact` - Appropriate time to offer Noah's contact
  - `show_code` - Technical query needs code display
  - `send_resume` - User wants resume
  - `log_feedback` - User provided feedback
- Stores plan in state for next node
- **Separation of concerns**: Planning â‰  Execution (testability!)

**Why it matters**:
- **Testability**: Verify correct actions planned without triggering real APIs
- **Flexibility**: Easy to add new action types (send SMS, create ticket, schedule call)
- **Clarity**: Single source of truth for ""what should happen""

**Planning logic**:
```python
def plan_actions(state):
    actions = []
    query_lower = state.query.lower()

    # Analytics request detection
    if any(word in query_lower for word in [""analytics"", ""metrics"", ""performance""]):
        actions.append({""type"": ""send_analytics"", ""tables"": [""messages"", ""retrieval_logs""]})

    # Contact offer timing (after 2+ turns, user engaged)
    if len(state.chat_history) >= 4 and state.role in [""hiring_manager_technical"", ""hiring_manager_nontechnical""]:
        actions.append({""type"": ""offer_contact"", ""method"": ""email""})

    state.stash(""planned_actions"", actions)
    return state
```

**Real-world pattern**: Salesforce Einstein uses similar planning to decide when to ""create lead"", ""send follow-up email"", or ""assign to sales rep"" based on conversation context.

---

### Node 6: apply_role_context - Personality Injector
**File**: `src/flows/core_nodes.py`
**Purpose**: Add role-specific enhancements to answer

**What it does**:
- Enhances base answer with role-appropriate elements:
  - **Software Developer**: Technical follow-ups (""Want to see the tests?""), architecture diagrams, code deep-dives
  - **Hiring Manager (Technical)**: Balance of technical + business value, Noah's contact offer after engagement
  - **Hiring Manager (Non-technical)**: Business ROI framing, success metrics, value propositions
  - **Just exploring**: Fun facts, casual tone, MMA fight links on keyword match
  - **Confess crush**: Stores confession, returns sweet acknowledgment (bypasses RAG entirely!)
- Adds follow-up prompts that invite deeper exploration
- Adjusts enthusiasm level per `CONVERSATION_PERSONALITY.md`

**Why it matters**:
- **User-centric**: Same knowledge, different presentation per audience (technical vs business framing)
- **Engagement**: Invites continued conversation with relevant prompts
- **Brand consistency**: Maintains warm, helpful personality across all interactions

**Role adaptation example**:
```python
def apply_role_context(state, rag_engine):
    base_answer = state.answer
    role = state.role

    if role == ""software_developer"":
        # Add technical follow-ups
        followups = [
            ""Want to see the retrieval code?"",
            ""Curious about the pgvector implementation?"",
            ""Should I explain the prompt engineering strategy?""
        ]
        state.set_answer(base_answer + ""\n\n"" + ""\n"".join(f""- {q}"" for q in followups[:2]))

    elif role == ""hiring_manager_nontechnical"":
        # Add business value framing
        value_prop = ""\n\nThis same architecture reduces support costs by 40% at enterprise scale.""
        state.set_answer(base_answer + value_prop)

    return state
```

**Enterprise application**: Customer support bots adapt tone for VIP customers (""We apologize for the inconvenience..."") vs standard users (""Sorry about that!""), demonstrating same technical capability with different presentation.

---

### Node 7: execute_actions - Side Effects Executor
**File**: `src/flows/action_execution.py`
**Purpose**: Execute planned side effects (email, SMS, logging)

**What it does**:
- Reads action plan from state (created in node 5)
- Executes each action:
  - **Send email**: Via Resend API (resume, contact info)
  - **Send SMS**: Via Twilio API (notifications to Noah)
  - **Log to storage**: Analytics, feedback, confessions
  - **Generate signed URLs**: Supabase Storage for private resume access
- **Graceful degradation**: If Resend down â†’ log error, continue conversation
- Uses service factories that handle missing API keys (degraded mode)

**Why it matters**:
- **Reliability**: System doesn't crash if external services fail (99.9% uptime despite third-party dependencies)
- **Observability**: All side effects logged for debugging (""Email sent at 2025-10-16 14:32:15"")
- **Separation**: Generation complete before side effects (can retry actions independently)

**Graceful degradation pattern**:
```python
def execute_actions(state):
    actions = state.fetch(""planned_actions"", [])

    for action in actions:
        try:
            if action[""type""] == ""send_email"":
                resend = get_resend_service()  # Factory handles missing key
                if resend:
                    resend.send_email(...)
                    logger.info(""Email sent successfully"")
                else:
                    logger.warning(""Resend API key not configured, email skipped"")
                    # Continue anyway - don't block conversation!
        except Exception as e:
            logger.error(f""Action execution failed: {e}"")
            # Log error but don't crash

    return state
```

**Service factory pattern** (handles missing keys):
```python
def get_resend_service():
    if not os.getenv(""RESEND_API_KEY""):
        logger.warning(""Resend API key not set, email disabled"")
        return None  # Graceful degradation
    return ResendService()
```

**Real-world value**: Production systems must handle third-party failures gracefully. This pattern ensures core functionality (answering questions) works even when optional services (email, SMS) fail.

---

### Node 8: log_and_notify - Observability Reporter
**File**: `src/flows/core_nodes.py`
**Purpose**: Log interaction for analytics, monitoring, and continuous improvement

**What it does**:
- Logs to Supabase:
  - `messages` table: Query, answer, latency, tokens, success/failure, role, session_id
  - `retrieval_logs` table: Which chunks retrieved, similarity scores, grounding validation
- Tracks metrics:
  - Response latency (p50, p95, p99)
  - Token count (cost tracking: $0.15 input + $0.60 output per 1M tokens)
  - Success rate (% queries answered satisfactorily)
  - Grounding rate (% answers using retrieved context)
- Enables ""Display Analytics"" feature (real-time performance dashboard)

**Why it matters**:
- **Continuous improvement**: See which queries slow, which fail, which confuse users
- **Cost tracking**: Monitor OpenAI API spend per query (avg $0.004/query)
- **User insights**: Understand what people ask, how they engage, where they drop off
- **Debugging**: Trace any conversation through logs (""Show me session abc123 retrieval scores"")

**Logging structure**:
```python
def log_and_notify(state, session_id, latency_ms):
    # Log interaction
    message_id = supabase.table(""messages"").insert({
        ""session_id"": session_id,
        ""role_mode"": state.role,
        ""user_query"": state.query,
        ""assistant_answer"": state.answer,
        ""latency_ms"": latency_ms,
        ""token_count"": count_tokens(state.answer),
        ""success"": True,
        ""created_at"": datetime.utcnow()
    }).execute()

    # Log retrieval details for quality measurement
    for chunk in state.fetch(""retrieved_chunks"", []):
        supabase.table(""retrieval_logs"").insert({
            ""message_id"": message_id,
            ""chunk_id"": chunk[""id""],
            ""similarity_score"": chunk[""score""],
            ""grounded"": chunk[""used_in_answer""],  # LLM-as-judge validation
            ""created_at"": datetime.utcnow()
        }).execute()

    return state
```

**Observability metrics** (available via ""Display Analytics""):
- Total interactions: 2,847
- Avg latency: 1,450ms (target: <2s)
- Grounding rate: 94% (target: >90%)
- Cost per query: $0.004 (budgeted: <$0.01)

**Enterprise pattern**: Same logging powers production AI dashboards at Intercom (support bot performance), Salesforce (Einstein analytics), and OpenAI (ChatGPT usage metrics).

---

## How Nodes Work Together: Real Example

**User asks: ""How does vector search work?""**

```
1. handle_greeting
   âœ“ Not a greeting â†’ pass through

2. classify_query
   âœ“ Technical question: needs_longer_response=True, code_would_help=True

3. retrieve_chunks
   âœ“ Embed query: [0.123, -0.456, ...] (768 dimensions)
   âœ“ pgvector search: Returns 4 chunks about RAG, embeddings, pgvector (avg similarity 0.87)
   âœ“ Code snippets: Pulls vector search implementation from code_chunks/

4. generate_answer
   âœ“ Prompt: ""Explain vector search using these chunks, include code since technical query""
   âœ“ GPT-4o-mini generates: Paragraph explanation + code snippet
   âœ“ Temperature 0.4: Factual but readable (not robotic)

5. plan_actions
   âœ“ Technical query â†’ Plan: {show_code: true, offer_contact: false}

6. apply_role_context
   âœ“ Adds: ""Want to see the pgvector index configuration?""
   âœ“ Adjusts tone for developer audience (detailed, implementation-focused)

7. execute_actions
   âœ“ No external services needed â†’ pass through

8. log_and_notify
   âœ“ Log to messages: latency=1450ms, tokens=950, cost=$0.004
   âœ“ Log to retrieval_logs: 4 chunks used, avg similarity=0.87, grounded=true
```

**Final Response**: Detailed explanation + code snippet + technical follow-up prompt âœ…

---

## Key Design Patterns (What Makes This Production-Ready)

### Pattern 1: Immutable State Updates
Each node returns **new state** (or modified copy), never mutates directly:
```python
state.set_answer(""response"")  # Helper method creates new state
state.stash(""key"", value)      # Store metadata without mutation
```

**Why**: Enables time-travel debugging, easy rollback, predictable behavior.

### Pattern 2: Short-Circuiting
Greeting node can skip expensive RAG operations:
```python
if is_greeting:
    return greeting_answer  # Stop here, save $0.002 + 1s latency
# Otherwise continue to retrieval...
```

**Why**: 30-40% of queries are simple (""hi"", ""thanks"") â†’ don't waste resources.

### Pattern 3: Separation of Concerns
- Planning â‰  Execution (actions planned in node 5, executed in node 7)
- Classification â‰  Generation (intent detected early, used later)
- Generation â‰  Logging (answer created before observability)

**Why**: Testability, maintainability, clear ownership.

### Pattern 4: Graceful Degradation
External services can fail without crashing:
```python
resend = get_resend_service()  # Returns None if API key missing
if resend:
    resend.send_email(...)
else:
    logger.warning(""Email service unavailable"")
    # Continue anyway - core functionality preserved!
```

**Why**: 99.9% uptime despite third-party dependencies.

### Pattern 5: Observability First
Every step logged for continuous improvement:
```python
logger.info(f""Node {node_name} completed in {elapsed_ms}ms"")
```

**Why**: Can't improve what you don't measure. Logs enable debugging, optimization, quality measurement.

---

## Why This Architecture? (Design Decisions)

**Testability**: Each node can be tested independently (unit tests per node)
**Maintainability**: Each node file <200 lines, single responsibility (easy to understand)
**Observability**: Log at each stage, trace failures back to specific node
**Flexibility**: Easy to add new nodes (e.g., `translate_answer` for multilingual support)
**Performance**: Can skip expensive operations (greetings bypass RAG, saving 1s + $0.002)
**Education**: Clean separation demonstrates GenAI best practices (used as teaching tool)

---

## Enterprise Adaptation Examples

**Customer Support Bot** (Same 8 nodes, different data):
1. handle_greeting â†’ ""Hello! How can I help you today?""
2. classify_query â†’ Detect: billing question vs technical issue vs general inquiry
3. retrieve_chunks â†’ Search product docs + troubleshooting KB + billing policies
4. generate_answer â†’ GPT-4 synthesizes answer with links to help articles
5. plan_actions â†’ Create Zendesk ticket if unresolved, escalate to human if frustrated
6. apply_role_context â†’ VIP customers get ""white glove"" tone, standard users get efficient tone
7. execute_actions â†’ Create ticket via Zendesk API, send confirmation email
8. log_and_notify â†’ Track resolution rate, customer satisfaction, cost per ticket

**Result**: 40% reduction in support tickets, $200k/year savings.

---

**Internal Documentation Assistant** (Same 8 nodes, different access):
1. handle_greeting â†’ ""Hi [employee_name]! What can I help you find?""
2. classify_query â†’ Detect: HR policy vs engineering docs vs sales playbook
3. retrieve_chunks â†’ Search Confluence + Notion + SharePoint (per-department access control)
4. generate_answer â†’ Claude synthesizes answer with SSO-validated permissions
5. plan_actions â†’ Log usage per department, identify knowledge gaps
6. apply_role_context â†’ Engineering gets technical detail, HR gets policy summaries
7. execute_actions â†’ Track onboarding progress, trigger follow-up tasks
8. log_and_notify â†’ Measure time-to-answer, knowledge gap analysis

**Result**: 60% faster onboarding, 80% reduction in ""where do I find X?"" Slack messages.

---

## Want to Explore Further?

**Deep-dive options**:
- ""Show me the classify_query code"" - See intent detection logic
- ""Display the retrieval pipeline"" - Dive into pgvector search
- ""How does role context work?"" - Explore personality injection
- ""Show me the action execution code"" - See graceful degradation pattern
- ""Explain the state management"" - Learn immutable patterns
- ""What's the cost per query?"" - Break down economics

Each node demonstrates production GenAI patterns you can adapt for your own projects!"
"Give me an overview of the 5 core knowledge pillars","# ğŸ¯ Portfolia's 5 Core Knowledge Pillars

I'm designed as an educational guide through 5 interconnected areas that demonstrate how production GenAI systems work:

## 1ï¸âƒ£ Orchestration Layer
**What it covers**: My 22-node LangGraph pipeline, conversation state management, and quality safeguards
**Key topics**: Nodes, stages, ConversationState fields, edge routing, quality gates
**Depth available**: From high-level flow diagram to individual node implementations

## 2ï¸âƒ£ Full Tech Stack
**What it covers**: Frontend to backend to data layer architecture
**Key topics**: Next.js/Streamlit UI, Python backend, Supabase Postgres, Vercel serverless
**Depth available**: From component overview to code-level implementation details

## 3ï¸âƒ£ Enterprise Adaptation
**What it covers**: How assistants like me are customized for enterprise deployments
**Key topics**: Customer support bots, internal documentation assistants, multi-tenant architecture, compliance
**Depth available**: From use case examples to production patterns and security considerations

## 4ï¸âƒ£ Data Pipeline Management
**What it covers**: How knowledge is created, stored, searched, and maintained
**Key topics**: Embeddings, pgvector, chunking strategies, analytics tracking, data quality
**Depth available**: From conceptual explanation to SQL queries and embedding code

## 5ï¸âƒ£ Noah's Technical Background
**What it covers**: Noah's engineering journey, projects, certifications, and proof of skills
**Key topics**: Career transition, Python/AI skills, production system experience, GitHub portfolio
**Depth available**: From career summary to specific project deep-dives

---

## How Depth Works

Each pillar supports progressive exploration:

**Level 1 (Overview)**: 30-second summary, key concepts, why it matters
**Level 2 (Components)**: Individual pieces broken down, how they connect
**Level 3 (Implementation)**: Code, configurations, specific details
**Level 4 (Advanced)**: Edge cases, optimizations, lessons learned

**Example progression**:
- 'Tell me about orchestration' â†’ Level 1 overview of 22 nodes in 7 stages
- 'How does classification work?' â†’ Level 2 breakdown of Stage 2 nodes
- 'Show me the classify_intent code' â†’ Level 3 implementation details
- 'What edge cases does it handle?' â†’ Level 4 advanced safeguards

---

## Which pillar interests you most?"
"How do you use the current conversation as examples?","# ğŸ¯ Self-Referential Teaching: Using Our Conversation as a Live Example

## Core Principle

When explaining how I work, I should reference **what just happened in our conversation** to make concepts tangible. Instead of abstract explanations, I show the system in action.

---

## Examples of Self-Referential Explanations

### Explaining retrieve_chunks:
\"Right now, when you asked about retrieval, my retrieve_chunks node searched the architecture_kb with your query. It found 6 relevant chunks including this very KB entry with a similarity score of 0.87. That's why I can explain this so specifically â€” I retrieved the exact documentation about myself.\"

### Explaining classify_role_mode:
\"At the start of our conversation, when you selected option 2, my classify_role_mode node stored role_mode='hiring_manager_technical' in session_memory. Every answer since then has been tailored for your technical background â€” that's why I'm showing code examples and architecture details.\"

### Explaining compose_query:
\"Your question 'how do nodes work' was enhanced to '[hiring_manager_technical] how do nodes work orchestration architecture pipeline' before retrieval. The role prefix and accumulated topics improved my search accuracy.\"

### Explaining update_memory:
\"After this response, my update_memory node will store 'orchestration' as a topic we discussed. Next time you ask something, my compose_query will include that context to improve retrieval.\"

### Explaining conversation phase:
\"We're in the 'exploration' phase (turns 4-8) which is why I'm offering deeper details. In the 'discovery' phase (turns 1-3), I would have kept things higher-level.\"

---

## Why This Works

1. **Tangibility**: Abstract concepts become concrete when users see them happening
2. **Trust**: Users see the system is transparent about its operations
3. **Education**: Users learn GenAI patterns through direct observation
4. **Engagement**: Self-reference creates a sense of conversation, not lecture

---

## State Fields I Can Reference

When explaining myself, I can mention:
- **query**: The exact question that triggered this response
- **role_mode**: The role stored from their selection
- **retrieved_chunks**: What KB content I found
- **depth_level**: Why I'm showing this level of detail
- **conversation_phase**: Where we are in our journey
- **topics**: What we've discussed so far
- **similarity scores**: How well my retrieval matched their question

**This makes me a live case study of production GenAI systems, not just documentation about one.**"
"Explain the data pipeline overview","# ğŸ“Š Data Pipeline Management Overview

## What is my Data Pipeline?

My data pipeline is the system that creates, stores, searches, and maintains knowledge. Think of it as my memory infrastructure â€” it's how I remember things about Noah and how I find relevant information when you ask questions.

**Live Example**: Right now, when you asked this question, my pipeline searched through 400+ knowledge chunks and found this exact entry with a similarity score of ~0.85. That's the data pipeline in action!

---

## The 5-Stage Data Pipeline

```
Stage 1: Knowledge Ingestion
    â†“ CSV files â†’ chunking â†’ embedding generation
Stage 2: Vector Storage
    â†“ Supabase pgvector â†’ IVFFLAT indexes
Stage 3: Semantic Search
    â†“ Query embedding â†’ cosine similarity â†’ top-k retrieval
Stage 4: Context Assembly
    â†“ Chunk formatting â†’ deduplication â†’ ranking
Stage 5: Analytics & Quality
    â†“ Logging â†’ metrics â†’ continuous improvement
```

---

## Key Metrics

| Metric | Value | What It Means |
|--------|-------|---------------|
| Knowledge chunks | 400+ | Total pieces of information I can retrieve |
| Embedding dimensions | 1536 | Vector size for semantic matching |
| Search latency | ~280ms | Time to find relevant chunks |
| Similarity threshold | 0.3 | Minimum match score for inclusion |
| Top-k retrieval | 6 chunks | Maximum context per query |

---

## Why This Matters

**For Production Systems**: Every GenAI application needs a data pipeline. Whether it's customer support bots (searching help docs), internal assistants (searching company knowledge), or code assistants (searching codebases) â€” the patterns are the same.

**For Learning**: Understanding my data pipeline teaches you how to build any RAG system. The concepts transfer directly to enterprise applications.

---

## Go Deeper

- 'Explain embeddings' â†’ How text becomes searchable numbers
- 'How does vector search work?' â†’ The similarity matching algorithm
- 'Show me the chunking strategy' â†’ How knowledge is broken into pieces
- 'How do you track data quality?' â†’ Analytics and improvement loops"
"Explain embeddings and vector storage","# ğŸ§® Embeddings: How Text Becomes Searchable Numbers

## What are Embeddings?

Embeddings convert text into lists of numbers (vectors) where **similar meanings = similar numbers**. This lets me find relevant information even when you use different words than my knowledge base.

**Example**:
- 'How does RAG work?' â†’ [0.23, -0.45, 0.67, ..., 0.12] (1536 numbers)
- 'Explain retrieval augmented generation' â†’ [0.25, -0.43, 0.69, ..., 0.11] â† Very similar!
- 'What's the weather?' â†’ [-0.89, 0.12, -0.34, ..., 0.78] â† Very different!

**Live Example**: Your question about embeddings was converted to a 1536-dimensional vector. I compared it against all my knowledge base vectors and found this entry had the highest similarity score.

---

## The Embedding Process

```python
# 1. Text comes in (your question or KB entry)
text = 'How do embeddings work?'

# 2. OpenAI API converts to vector
response = openai.embeddings.create(
    model='text-embedding-3-small',  # Our model choice
    input=text
)
embedding = response.data[0].embedding  # 1536 floats

# 3. Store in Supabase pgvector
supabase.table('kb_chunks').insert({
    'content': text,
    'embedding': embedding  # vector(1536) column
})
```

---

## Vector Storage with pgvector

I use **Supabase Postgres with pgvector extension** to store and search embeddings:

**Why pgvector?**
- âœ… SQL interface (can JOIN with other tables)
- âœ… ACID transactions (data consistency)
- âœ… Managed hosting (Supabase handles maintenance)
- âœ… $25/month (vs $70/month Pinecone)

**The Schema**:
```sql
CREATE TABLE kb_chunks (
    id BIGSERIAL PRIMARY KEY,
    doc_id TEXT,           -- 'career_kb', 'architecture_kb'
    section TEXT,          -- Entry title
    content TEXT,          -- Full Q&A text
    embedding vector(1536), -- The magic numbers!
    metadata JSONB         -- Source, category, etc.
);

-- IVFFLAT index for fast search
CREATE INDEX kb_chunks_embedding_idx
ON kb_chunks
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);
```

---

## Embedding Model Choice

| Model | Dimensions | Cost | Why I Use It |
|-------|-----------|------|--------------|
| text-embedding-3-small | 1536 | $0.00002/1K tokens | âœ… Best value |
| text-embedding-3-large | 3072 | $0.00013/1K tokens | More dimensions, not needed |
| ada-002 (legacy) | 1536 | $0.0001/1K tokens | 5x more expensive |

**Cost Example**: At 500 queries/day Ã— $0.00002 = $0.01/day = $3/month

---

## Key Concepts

**Cosine Similarity**: Measures angle between vectors (0Â° = identical, 90Â° = unrelated, 180Â° = opposite)
**Dimensionality**: More dimensions = more nuance, but 1536 is optimal for most use cases
**Semantic vs Lexical**: Embeddings find meaning, not just matching words

---

## Enterprise Pattern

Same embedding approach powers:
- **Customer support**: Embed help articles, find relevant docs for user questions
- **Code search**: Embed code files, find similar implementations
- **Document QA**: Embed contracts/policies, answer compliance questions"
"Explain the vector search process","# ğŸ” Vector Search: Finding Relevant Knowledge

## How Does Semantic Search Work?

When you ask me a question, I find relevant answers by comparing your question's embedding to all my stored knowledge embeddings. The closest matches become my context for answering.

**Live Example**: For your question about vector search, I:
1. Converted it to a 1536-dimensional vector
2. Compared against 400+ KB chunk vectors
3. Found this entry with similarity ~0.87
4. Used it to generate this response

---

## The Search Algorithm

```sql
-- The core pgvector query
SELECT
    id, content, doc_id, section,
    1 - (embedding <=> query_embedding) AS similarity
FROM kb_chunks
WHERE 1 - (embedding <=> query_embedding) > 0.3  -- Threshold
ORDER BY embedding <=> query_embedding           -- Nearest first
LIMIT 6;                                         -- Top-k
```

**Key Operators**:
- `<=>` : Cosine distance (0 = identical, 2 = opposite)
- `1 - distance` : Convert to similarity (1 = identical, 0 = different)

---

## The Search Pipeline

```
Your Question
    â†“
[1] Generate Query Embedding (200ms)
    OpenAI text-embedding-3-small
    â†“
[2] Vector Similarity Search (280ms)
    pgvector cosine distance
    IVFFLAT index for O(âˆšn) speed
    â†“
[3] Apply Threshold Filter
    Only chunks with similarity > 0.3
    â†“
[4] MMR Diversification
    Avoid redundant chunks
    â†“
[5] Return Top-6 Chunks
    With similarity scores and metadata
```

---

## Performance Optimization

**Without Index (Sequential Scan)**:
- Compares query to ALL vectors
- O(n) complexity
- 1000ms for 10K chunks

**With IVFFLAT Index**:
- Partitions vectors into clusters
- Only searches relevant clusters
- O(âˆšn) complexity
- 280ms for 10K chunks

```sql
-- The index that makes it fast
CREATE INDEX kb_chunks_embedding_idx
ON kb_chunks
USING ivfflat (embedding vector_cosine_ops)
WITH (lists = 100);  -- âˆš10000 = 100 clusters optimal
```

---

## Similarity Score Interpretation

| Score | Meaning | Action |
|-------|---------|--------|
| 0.85+ | Excellent match | High confidence retrieval |
| 0.70-0.84 | Good match | Include in context |
| 0.50-0.69 | Moderate match | Include if needed |
| 0.30-0.49 | Weak match | Include for coverage |
| <0.30 | Poor match | Exclude from context |

---

## Advanced: MMR Diversification

**Problem**: Top-6 chunks might all say the same thing
**Solution**: Maximal Marginal Relevance (MMR) balances relevance with diversity

```python
def mmr_rerank(query_embedding, chunks, lambda_param=0.5):
    '''Select chunks that are relevant but not redundant'''
    selected = []
    remaining = chunks.copy()

    while len(selected) < 6 and remaining:
        # Score = relevance - redundancy
        scores = []
        for chunk in remaining:
            relevance = similarity(query_embedding, chunk.embedding)
            redundancy = max(similarity(chunk.embedding, s.embedding)
                           for s in selected) if selected else 0
            mmr_score = lambda_param * relevance - (1-lambda_param) * redundancy
            scores.append((chunk, mmr_score))

        best = max(scores, key=lambda x: x[1])
        selected.append(best[0])
        remaining.remove(best[0])

    return selected
```

---

## Enterprise Applications

| Use Case | What's Searched | Chunk Size |
|----------|-----------------|------------|
| Customer Support | Help docs, FAQs | 200-500 tokens |
| Legal Research | Contracts, case law | 500-1000 tokens |
| Code Search | Functions, classes | Full function |
| Internal Docs | Confluence, Notion | 300-600 tokens |"
"Explain the chunking strategy","# ğŸ“„ Chunking Strategy: Breaking Knowledge Into Pieces

## Why Chunk?

LLMs have context limits. I can't feed my entire knowledge base into GPT-4o-mini. Instead, I break knowledge into **chunks** that are:
- Small enough to fit in context (6 chunks Ã— ~500 tokens = 3000 tokens)
- Large enough to be meaningful (complete Q&A pairs, not fragments)
- Optimized for retrieval (each chunk should be self-contained)

---

## My Chunking Approach

I use **Q&A Pair Chunking** â€” each question-answer pair from my knowledge base becomes one chunk:

```
Chunk 1: 'What is the tech stack?' â†’ Full answer about Python, LangGraph, etc.
Chunk 2: 'How does RAG work?' â†’ Complete RAG explanation
Chunk 3: 'What are Noah's skills?' â†’ Full skills list
```

**Why this works for me**:
- Questions are natural retrieval anchors
- Answers are complete and self-contained
- No risk of cutting off mid-sentence
- Clear metadata (source KB, question as title)

---

## Chunking Strategies Compared

| Strategy | Description | Best For | Drawbacks |
|----------|-------------|----------|-----------|
| **Q&A Pairs** (mine) | Keep question-answer together | FAQs, knowledge bases | Requires structured source |
| **Fixed Size** | Split every 500 tokens | Long documents | May split mid-sentence |
| **Semantic** | Split at topic boundaries | Articles, books | Complex to implement |
| **Sentence** | One sentence per chunk | Legal docs | Too granular, loses context |
| **Recursive** | Split, then split again | Code files | Multiple passes needed |

---

## The Ingestion Process

```python
# scripts/migrate_data_to_supabase.py

def process_kb_file(csv_path):
    '''Convert CSV Q&A pairs into embedded chunks'''
    df = pd.read_csv(csv_path)
    doc_id = csv_path.stem  # 'career_kb', 'technical_kb'

    for _, row in df.iterrows():
        question = row['Question']
        answer = row['Answer']

        # Combine for embedding (question helps retrieval)
        full_content = f'Question: {question}\n\nAnswer: {answer}'

        # Generate embedding
        embedding = openai.embeddings.create(
            model='text-embedding-3-small',
            input=full_content
        ).data[0].embedding

        # Store chunk
        supabase.table('kb_chunks').insert({
            'doc_id': doc_id,
            'section': question,  # Chunk title
            'content': full_content,
            'embedding': embedding,
            'metadata': {'source': csv_path.name}
        })
```

---

## Chunk Size Analysis

**My chunks**: Average ~400 tokens per Q&A pair

| Metric | Value |
|--------|-------|
| Min chunk size | 50 tokens (short answers) |
| Max chunk size | 2000 tokens (detailed explanations) |
| Average | ~400 tokens |
| Total chunks | 400+ across all KBs |

**Context budget**: 6 chunks Ã— 400 tokens = 2400 tokens for retrieval context
**Remaining**: GPT-4o-mini has 128K limit, so plenty of room!

---

## Overlap Considerations

**I don't use overlap** because Q&A pairs are self-contained.

For other chunking strategies, overlap prevents losing context at boundaries:

```
Without overlap:
  [Chunk 1: tokens 1-500] [Chunk 2: tokens 501-1000]
  â†‘ Sentence might be cut here!

With 50-token overlap:
  [Chunk 1: tokens 1-500] [Chunk 2: tokens 451-950]
  â†‘ Overlapping region ensures context preserved
```

---

## Knowledge Base Sources

| KB File | Description | Chunk Count |
|---------|-------------|-------------|
| career_kb.csv | Noah's background | ~30 chunks |
| technical_kb.csv | Tech stack, features | ~50 chunks |
| architecture_kb.csv | Diagrams, node pipeline | ~20 chunks |
| personality_kb.csv | Conversation style | ~15 chunks |
| conversation_guidance_kb.csv | Off-topic handling | ~10 chunks |

---

## Enterprise Patterns

| Application | Chunk Strategy | Size |
|-------------|---------------|------|
| Customer Support | FAQ pairs | 200-400 tokens |
| Legal Docs | Clause-level | 500-1000 tokens |
| Code Search | Function-level | Full function |
| Long Reports | Section headers | 800-1200 tokens |"
"How do you track analytics and data quality","# ğŸ“ˆ Analytics & Data Quality Tracking

## What Do I Track?

Every conversation generates data that helps Noah improve my performance. I track **6 categories of metrics**:

1. **Interaction Logs**: Every query and response
2. **Retrieval Quality**: Which chunks matched and how well
3. **Performance Metrics**: Latency, tokens, costs
4. **User Engagement**: Session length, follow-up patterns
5. **Quality Signals**: Grounding status, relevance scores
6. **Feedback**: User ratings when provided

---

## The Analytics Pipeline

```
User Query
    â†“
[1] Log Query (async)
    session_id, query, role, timestamp
    â†“
[2] Log Retrieval (async)
    chunk_ids, similarity_scores, doc_ids
    â†“
[3] Log Response (async)
    answer, latency_ms, tokens, cost
    â†“
[4] Aggregate Metrics
    Daily rollups, trend analysis
    â†“
[5] Quality Monitoring
    Alerts on degradation
```

---

## Database Schema for Analytics

```sql
-- Core interaction log
CREATE TABLE messages (
    id BIGSERIAL PRIMARY KEY,
    session_id UUID NOT NULL,
    role_mode TEXT,
    query TEXT NOT NULL,
    answer TEXT NOT NULL,
    query_type TEXT,        -- technical, career, action_request
    latency_ms INTEGER,
    tokens_prompt INTEGER,
    tokens_completion INTEGER,
    success BOOLEAN,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Retrieval tracking
CREATE TABLE retrieval_logs (
    id BIGSERIAL PRIMARY KEY,
    message_id BIGINT REFERENCES messages(id),
    topk_ids BIGINT[],      -- Array of chunk IDs
    scores FLOAT[],         -- Similarity scores
    grounded BOOLEAN,       -- Was answer grounded?
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

---

## Key Metrics Dashboard

| Metric | Target | Current | Status |
|--------|--------|---------|--------|
| Avg Latency | <3s | 2.3s | âœ… |
| Grounding Rate | >90% | 94% | âœ… |
| Avg Similarity | >0.7 | 0.78 | âœ… |
| Success Rate | >85% | 87% | âœ… |
| Cost per Query | <$0.001 | $0.0004 | âœ… |

---

## Quality Monitoring

**Automated Checks (run with each query)**:
- Retrieval similarity > 0.3 (else mark as 'ungrounded')
- Response contains no raw metrics (metrics stripping)
- Response not identical to previous turn (novelty check)
- Response addresses query keywords (relevance check)

**Daily Maintenance (scripts/daily_maintenance.py)**:
- Compute 7-day rolling averages
- Flag sessions with low engagement
- Identify knowledge gaps (low-similarity queries)
- Generate improvement recommendations

---

## LangSmith Integration

For deeper observability, I use LangSmith tracing:

```python
# Every LLM call is traced
with langsmith.trace('generate_draft') as trace:
    response = openai.chat.completions.create(...)
    trace.log_output(response)
    trace.log_metadata({
        'tokens': response.usage.total_tokens,
        'latency_ms': elapsed_ms,
        'grounded': grounding_status
    })
```

**What LangSmith Shows**:
- Full prompt/response pairs
- Token counts and costs
- Latency breakdowns
- Error traces and debugging

---

## Continuous Improvement Loop

```
[1] Identify Low-Quality Queries
    Similarity < 0.5 = knowledge gap
    â†“
[2] Cluster Similar Failures
    'leadership experience' + 'team management' = missing topic
    â†“
[3] Add KB Entries
    New Q&A pair addressing gap
    â†“
[4] Regenerate Embeddings
    scripts/migrate_data_to_supabase.py
    â†“
[5] Validate Improvement
    Re-run test queries, check similarity increase
```

---

## Example SQL Queries

**Find knowledge gaps**:
```sql
SELECT m.query, AVG(unnest(r.scores)) as avg_sim
FROM messages m
JOIN retrieval_logs r ON m.id = r.message_id
WHERE m.created_at > NOW() - INTERVAL '7 days'
GROUP BY m.query
HAVING AVG(unnest(r.scores)) < 0.5
ORDER BY avg_sim ASC
LIMIT 10;
```

**Performance trend**:
```sql
SELECT DATE(created_at),
       AVG(latency_ms) as avg_latency,
       COUNT(*) as queries
FROM messages
WHERE created_at > NOW() - INTERVAL '30 days'
GROUP BY DATE(created_at)
ORDER BY DATE(created_at);
```"
"Explain the ConversationState fields","# ğŸ“‹ ConversationState: The Data Flowing Through My Pipeline

## Overview

ConversationState is a TypedDict with ~40 fields that flows through all 22 nodes. Each node reads what it needs and writes its outputs. Think of it as a clipboard passed down an assembly line.

**File**: `assistant/state/conversation_state.py`

---

## Core Conversation Fields

| Field | Type | Purpose | Example |
|-------|------|---------|---------|
| query | str | User's current question | \"explain the nodes\" |
| role | str | Display role name | \"Hiring Manager (technical)\" |
| role_mode | str | Normalized role key | \"hiring_manager_technical\" |
| session_id | str | Unique session identifier | \"abc123-def456\" |
| chat_history | List[Dict] | Full conversation history | [{\"role\": \"user\", \"content\": \"...\"}] |

**Live Example**: Right now, my state has query='explain ConversationState' and role_mode='hiring_manager_technical'.

---

## Classification Fields

| Field | Type | Purpose |
|-------|------|---------|
| query_type | str | technical, career, analytics, greeting, menu_selection |
| is_greeting | bool | True if simple hello (triggers short-circuit) |
| topic_focus | str | architecture, data, testing, enterprise, etc. |
| intent_confidence | float | 0-1 confidence in classification |
| show_technical_depth | bool | Whether to include code/architecture |
| relate_to_enterprise | bool | Whether to tie back to enterprise value |

**Live Example**: Your question was classified as query_type='technical' with show_technical_depth=True.

---

## Retrieval Fields

| Field | Type | Purpose |
|-------|------|---------|
| composed_query | str | Enhanced query sent to retrieval |
| retrieved_chunks | List[Dict] | Top-k chunks from pgvector search |
| retrieval_scores | List[float] | Similarity scores for each chunk |
| grounding_status | str | ok, insufficient, or gap |
| code_snippets | List[Dict] | Code files relevant to query |

**Live Example**: My composed_query added your role prefix and accumulated topics to improve search accuracy.

---

## Generation Fields

| Field | Type | Purpose |
|-------|------|---------|
| draft_answer | str | Initial LLM response (before formatting) |
| answer | str | Final formatted response |
| hallucination_safe | bool | True if answer grounded in retrieved content |
| depth_level | int | 1=overview, 2=guided detail, 3=deep dive |

---

## Memory Fields (session_memory Dict)

| Nested Field | Type | Purpose |
|--------------|------|---------|
| persona_hints.role_mode | str | Stored role for all future turns |
| topics | List[str] | Last 10 topics discussed |
| entities | Dict | Extracted company, role, timeline |
| explored_pillars | List[str] | Which knowledge pillars we've covered |
| affinity_scores | Dict | technical_relevance_score, enterprise_relevance_score |

**Live Example**: My session_memory tracks that we've explored 'orchestration' and 'architecture', and your technical_relevance_score is high.

---

## Action Fields

| Field | Type | Purpose |
|-------|------|---------|
| pending_actions | List[Dict] | Actions planned but not executed |
| executed_actions | List[Dict] | Actions completed this turn |
| followup_prompts | List[str] | Suggested next questions |
| hiring_signals | List[str] | Detected interest indicators |

---

## Quality Validation Fields

| Field | Type | Purpose |
|-------|------|---------|
| conversation_phase | str | discovery, exploration, synthesis, extended |
| answer_quality_warning | str | Quality issues detected |
| conversation_guidance_needed | List[str] | stuck_need_redirection, suggest_synthesis, etc. |
| is_repeated_query | bool | True if user asked same question before |

---

## How Nodes Use State

**Reading**: `query = state.get(\"query\", \"\")`
**Writing**: `return {\"answer\": generated_text}` (partial update)

LangGraph merges partial updates automatically â€” nodes only return what they changed.

**Want to see how a specific field flows through the pipeline? I can trace any field through all 22 nodes.**"
